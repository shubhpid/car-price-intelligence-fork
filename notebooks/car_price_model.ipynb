{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "accelerator": "GPU",
  "colab": {"gpuType": "T4", "provenance": []}
 },
 "cells": [

  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Car Price Prediction — XGBoost + SHAP\n",
    "**Runtime:** GPU → T4  \n",
    "**Input:** `cleaned_cars.csv` from Google Drive  \n",
    "**Outputs:** `car_price_model.pkl` · `shap_data.pkl` · `feature_meta.pkl` · `shap_summary.png`\n",
    "\n",
    "| Cell | Purpose |\n",
    "|------|---------|\n",
    "| **Cell 1** | Feature engineering + time-based train/test split |\n",
    "| **Cell 2** | XGBoost training (GPU) + evaluation + feature importances |\n",
    "| **Cell 3** | SHAP analysis + `explain_prediction()` function |"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Verify T4 is attached ─────────────────────────────────────────────────────\n",
    "import subprocess, sys\n",
    "r = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total',\n",
    "                    '--format=csv,noheader'], capture_output=True, text=True)\n",
    "print('GPU:', r.stdout.strip() if r.returncode == 0 else 'NOT FOUND — switch runtime to T4')\n",
    "\n",
    "# ── Install / upgrade packages ────────────────────────────────────────────────\n",
    "!pip install -q xgboost shap joblib scikit-learn pandas numpy matplotlib\n",
    "print('Packages ready')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib, shap, xgboost as xgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(f'XGBoost {xgb.__version__} | SHAP {shap.__version__}')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Mount Drive ───────────────────────────────────────────────────────────────\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "CSV_PATH    = '/content/drive/MyDrive/car_data/cleaned_cars.csv'\n",
    "MODELS_DIR  = Path('/content/drive/MyDrive/car_data/models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.rename(columns={'manufacturer': 'make'})\n",
    "\n",
    "print(f'Loaded: {df.shape}  —  columns: {list(df.columns)}')"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-cell1",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Feature Engineering"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-eng",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUXURY_MAKES = {\n",
    "    'bmw', 'mercedes-benz', 'audi', 'lexus', 'porsche', 'cadillac',\n",
    "    'lincoln', 'infiniti', 'acura', 'volvo', 'land rover', 'jaguar', 'genesis'\n",
    "}\n",
    "\n",
    "# ── Category columns to label-encode ─────────────────────────────────────────\n",
    "CAT_COLS = ['make', 'model', 'condition', 'fuel', 'transmission',\n",
    "            'drive', 'type', 'state', 'region', 'paint_color',\n",
    "            'title_status', 'cylinders']\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'car_age', 'log_odometer', 'mileage_per_year', 'is_luxury', 'month',\n",
    "    'make', 'model', 'condition', 'fuel', 'transmission', 'drive',\n",
    "    'type', 'state', 'region', 'paint_color', 'title_status', 'cylinders',\n",
    "    'lat', 'long',\n",
    "]\n",
    "\n",
    "def engineer(df: pd.DataFrame, cat_codes: dict = None):\n",
    "    \"\"\"Build features. Pass cat_codes=None at train time (fits codes).\n",
    "    Pass saved cat_codes dict at inference time (maps unseen → -1).\"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    # ── Numeric features ──────────────────────────────────────────────────────\n",
    "    d['year']       = pd.to_numeric(d['year'],      errors='coerce')\n",
    "    d['odometer']   = pd.to_numeric(d['odometer'],  errors='coerce')\n",
    "    d['lat']        = pd.to_numeric(d['lat'],        errors='coerce')\n",
    "    d['long']       = pd.to_numeric(d['long'],       errors='coerce')\n",
    "\n",
    "    d['car_age']         = (2024 - d['year']).clip(0, 50).fillna(10)\n",
    "    d['log_odometer']    = np.log1p(d['odometer'].fillna(0))\n",
    "    d['mileage_per_year']= d['odometer'].fillna(0) / d['car_age'].replace(0, 1)\n",
    "    d['is_luxury']       = d['make'].str.lower().isin(LUXURY_MAKES).astype(int)\n",
    "\n",
    "    # month already exists from cleaning notebook; fallback parse if missing\n",
    "    if 'month' not in d.columns or d['month'].isna().all():\n",
    "        d['posting_date'] = pd.to_datetime(d.get('posting_date'), errors='coerce', utc=True)\n",
    "        d['month'] = d['posting_date'].dt.month.fillna(6)\n",
    "    d['month'] = d['month'].fillna(6).astype(int)\n",
    "\n",
    "    # ── Fill geo nulls with median ────────────────────────────────────────────\n",
    "    d['lat']  = d['lat'].fillna(d['lat'].median())\n",
    "    d['long'] = d['long'].fillna(d['long'].median())\n",
    "\n",
    "    # ── Categorical encoding ──────────────────────────────────────────────────\n",
    "    fitted_codes = {} if cat_codes is None else cat_codes\n",
    "    for col in CAT_COLS:\n",
    "        if col not in d.columns:\n",
    "            d[col] = -1\n",
    "            continue\n",
    "        d[col] = d[col].fillna('unknown').astype(str).str.lower().str.strip()\n",
    "        if cat_codes is None:\n",
    "            # Training: fit label codes\n",
    "            d[col] = d[col].astype('category')\n",
    "            fitted_codes[col] = dict(enumerate(d[col].cat.categories))\n",
    "            fitted_codes[col] = {v: k for k, v in fitted_codes[col].items()}\n",
    "            d[col] = d[col].cat.codes\n",
    "        else:\n",
    "            # Inference: map known, unknown → -1\n",
    "            d[col] = d[col].map(cat_codes.get(col, {})).fillna(-1).astype(int)\n",
    "\n",
    "    available = [c for c in FEATURE_COLS if c in d.columns]\n",
    "    return d[available], available, fitted_codes\n",
    "\n",
    "\n",
    "# ── Target: log1p(price) ──────────────────────────────────────────────────────\n",
    "df = df.dropna(subset=['price'])\n",
    "df = df[df['price'] > 0]\n",
    "y_raw = df['price'].values\n",
    "y_log = np.log1p(y_raw)\n",
    "\n",
    "# ── Time-based split (sort by posting_date — avoids data leakage) ─────────────\n",
    "df['posting_date'] = pd.to_datetime(df.get('posting_date'), errors='coerce', utc=True)\n",
    "df = df.sort_values('posting_date').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df) * 0.80)\n",
    "train_df, test_df = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "y_train_log = np.log1p(train_df['price'].values)\n",
    "y_test_log  = np.log1p(test_df['price'].values)\n",
    "\n",
    "print(f'Train: {len(train_df):,}  |  Test: {len(test_df):,}')\n",
    "print(f'Train date range: {train_df[\"posting_date\"].min()} → {train_df[\"posting_date\"].max()}')\n",
    "print(f'Test  date range: {test_df[\"posting_date\"].min()}  → {test_df[\"posting_date\"].max()}')\n",
    "\n",
    "# ── Build feature matrices ────────────────────────────────────────────────────\n",
    "X_train, feature_names, cat_codes = engineer(train_df)\n",
    "X_test,  _,            _          = engineer(test_df, cat_codes)\n",
    "\n",
    "print(f'\\nFeatures ({len(feature_names)}): {feature_names}')\n",
    "print(f'X_train: {X_train.shape}  |  X_test: {X_test.shape}')"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-cell2",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — XGBoost Training (T4 GPU)"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Auto-detect GPU param style (XGBoost >= 2.0 vs older) ────────────────────\n",
    "xgb_major = int(xgb.__version__.split('.')[0])\n",
    "gpu_params = {'device': 'cuda'} if xgb_major >= 2 else {'tree_method': 'gpu_hist'}\n",
    "print(f'XGBoost {xgb.__version__} — using GPU params: {gpu_params}')\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators        = 500,\n",
    "    learning_rate       = 0.05,\n",
    "    max_depth           = 6,\n",
    "    subsample           = 0.8,\n",
    "    colsample_bytree    = 0.8,\n",
    "    early_stopping_rounds = 50,\n",
    "    eval_metric         = 'rmse',\n",
    "    random_state        = 42,\n",
    "    **gpu_params,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train_log,\n",
    "    eval_set=[(X_test, y_test_log)],\n",
    "    verbose=50,\n",
    ")\n",
    "print(f'\\nBest iteration: {model.best_iteration}')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluate on original price scale ─────────────────────────────────────────\n",
    "y_pred_log  = model.predict(X_test)\n",
    "y_pred      = np.expm1(y_pred_log)\n",
    "y_test_orig = np.expm1(y_test_log)\n",
    "\n",
    "mae  = mean_absolute_error(y_test_orig, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "mape = np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100\n",
    "\n",
    "print('=== Test-set metrics (original $ scale) ===')\n",
    "print(f'  MAE:  ${mae:,.0f}')\n",
    "print(f'  RMSE: ${rmse:,.0f}')\n",
    "print(f'  MAPE: {mape:.2f}%')\n",
    "\n",
    "# ── Top-10 feature importances ────────────────────────────────────────────────\n",
    "imp = pd.Series(model.feature_importances_, index=feature_names)\n",
    "print('\\n=== Top-10 feature importances (gain) ===')\n",
    "print(imp.sort_values(ascending=False).head(10).to_string())\n",
    "\n",
    "# ── Save model + feature meta ─────────────────────────────────────────────────\n",
    "joblib.dump(model, MODELS_DIR / 'car_price_model.pkl')\n",
    "\n",
    "feature_meta = {\n",
    "    'feature_names': feature_names,\n",
    "    'cat_codes':     cat_codes,\n",
    "    'luxury_makes':  list(LUXURY_MAKES),\n",
    "    'lat_median':    float(df['lat'].median()),\n",
    "    'long_median':   float(df['long'].median()),\n",
    "}\n",
    "joblib.dump(feature_meta, MODELS_DIR / 'feature_meta.pkl')\n",
    "print('\\nSaved: car_price_model.pkl  |  feature_meta.pkl')"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-cell3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — SHAP Analysis"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sample 500 rows (T4 is overkill here; keep it fast) ──────────────────────\n",
    "np.random.seed(42)\n",
    "sample_idx    = np.random.choice(len(X_test), size=min(500, len(X_test)), replace=False)\n",
    "X_test_sample = X_test.iloc[sample_idx].reset_index(drop=True)\n",
    "\n",
    "explainer   = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f'SHAP values shape: {shap_values.shape}')   # (500, n_features)\n",
    "print('Expected value (log scale):', round(explainer.expected_value, 4),\n",
    "      f'→ ${np.expm1(explainer.expected_value):,.0f}')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary bar plot ─────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "shap.summary_plot(shap_values, X_test_sample,\n",
    "                  feature_names=feature_names,\n",
    "                  plot_type='bar', show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'shap_summary.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: shap_summary.png')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-explain-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── explain_prediction() — will be imported by FastAPI backend ────────────────\n",
    "def explain_prediction(row_dict: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Given a raw car listing dict (make, year, odometer, …),\n",
    "    return the top-3 SHAP contributors as:\n",
    "      [{feature, value, impact, direction}, ...]\n",
    "    direction is 'increases price' or 'decreases price'.\n",
    "    \"\"\"\n",
    "    row_df = pd.DataFrame([row_dict])\n",
    "    X_row, _, _ = engineer(row_df, cat_codes)           # use fitted codes from training\n",
    "    sv = explainer.shap_values(X_row)[0]                # shape: (n_features,)\n",
    "\n",
    "    top3_idx = np.argsort(np.abs(sv))[::-1][:3]\n",
    "    return [\n",
    "        {\n",
    "            'feature':   feature_names[i],\n",
    "            'value':     float(X_row.iloc[0, i]),\n",
    "            'impact':    round(float(abs(sv[i])), 4),\n",
    "            'direction': 'increases price' if sv[i] > 0 else 'decreases price',\n",
    "        }\n",
    "        for i in top3_idx\n",
    "    ]\n",
    "\n",
    "\n",
    "# ── Quick sanity test ─────────────────────────────────────────────────────────\n",
    "sample_row = test_df.iloc[0].to_dict()\n",
    "explanation = explain_prediction(sample_row)\n",
    "print('explain_prediction() output:')\n",
    "for e in explanation:\n",
    "    print(f\"  {e['feature']:20s}  {e['direction']:20s}  impact={e['impact']}\")"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save SHAP data (explainer + sample) for offline use ──────────────────────\n",
    "joblib.dump(\n",
    "    {'explainer': explainer, 'X_test_sample': X_test_sample, 'shap_values': shap_values},\n",
    "    MODELS_DIR / 'shap_data.pkl'\n",
    ")\n",
    "print('Saved: shap_data.pkl')\n",
    "\n",
    "# ── Download all artefacts to browser ────────────────────────────────────────\n",
    "from google.colab import files\n",
    "import shutil, os\n",
    "\n",
    "LOCAL = Path('/content/model_artefacts')\n",
    "LOCAL.mkdir(exist_ok=True)\n",
    "\n",
    "for fname in ['car_price_model.pkl', 'feature_meta.pkl', 'shap_data.pkl', 'shap_summary.png']:\n",
    "    shutil.copy(MODELS_DIR / fname, LOCAL / fname)\n",
    "    files.download(str(LOCAL / fname))\n",
    "    print(f'  Downloaded: {fname}')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n===== Training complete =====')\n",
    "print(f'  Model:       car_price_model.pkl')\n",
    "print(f'  Feature meta: feature_meta.pkl  ({len(feature_names)} features)')\n",
    "print(f'  SHAP data:   shap_data.pkl')\n",
    "print(f'  SHAP plot:   shap_summary.png')\n",
    "print(f'\\n  MAE  = ${mae:,.0f}')\n",
    "print(f'  RMSE = ${rmse:,.0f}')\n",
    "print(f'  MAPE = {mape:.2f}%')\n",
    "print(f'\\n  Copy artefacts to: car-price-intelligence/models/')"
   ]
  }

 ]
}
